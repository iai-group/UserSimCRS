output_name: "iai_moviebot_dual_llm_movielens"
debug: False
fix_random_seed: False
num_simulated_dialogues: 100

### AGENT CONFIGURATION ###
agent_class_path: "sample_agents.moviebot_agent.MovieBotAgent"
agent_id: "IAI MovieBot"
# By default, the agent has an HTTP API.
agent_uri: "http://127.0.0.1:5002"

### SIMULATOR CONFIGURATION ###
simulator_class_path: "usersimcrs.simulator.llm.dual_prompt_user_simulator.DualPromptUserSimulator"
simulator_id: "Dual_LLM_nemo_MovieLens"

domain: data/domains/moviebot.yaml
item_type: "movie"

collection_db_path: "data/item_collections.db"
collection_name: "movies_w_keywords"
items: data/item_collections/movielens-25m-sample/movies_w_keywords.csv
id_col: movieId
domain_mapping:
  title:
    slot: TITLE
  genres:
    slot: GENRE
    multi-valued: True
    delimiter: "|"
  keywords:
    slot: KEYWORD
    multi-valued: True
    delimiter: "|"
ratings: data/item_collections/movielens-25m-sample/ratings.csv
historical_ratings_ratio: 0.8

task_definition: "You are a USER discussing with an ASSISTANT. Given the conversation history, you need to generate the next USER message in the most natural way possible. The conversation is about getting a recommendation according to the REQUIREMENTS. You must fulfill all REQUIREMENTS as the conversation progresses (you don't need to fulfill them all at once). After getting all the necessary information, you can terminate the conversation by sending 'exit'. You may also terminate the conversation is stuck in a loop or the ASSISTANT is not helpful by sending 'quit'. "

stop_definition: "As a USER interacting with an ASSISTANT to receive a recommendation, analyze the conversation history to determine if it is progressing productively. If the conversation has been stuck in a loop with repeated misunderstandings across multiple turns, return 'FALSE' to indicate the conversation should be terminated. Otherwise, return 'TRUE' to indicate that the conversation should continue. Only return 'TRUE' or 'FALSE' without any additional information."

llm_interface_class_path: "usersimcrs.simulator.llm.interfaces.ollama_interface.OllamaLLMInterface"
llm_interface_args:
  configuration_path: config/llm_interface/config_ollama_default.yaml
