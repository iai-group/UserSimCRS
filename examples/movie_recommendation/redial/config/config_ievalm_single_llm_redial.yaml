output_name: "ievalm_single_llm_redial"
debug: False
fix_random_seed: False
num_simulated_dialogues: 100

### AGENT CONFIGURATION ###
agent_class_path: "crs_agent_wrapper.ievalm_agent.iEvaLMAgent"
agent_id: "BARCOR_ReDial"
# By default, the agent has an HTTP API.
agent_uri: "http://127.0.0.1:5006"
agent_config:
  stop_intent:
    class_path: "dialoguekit.core.intent.Intent"
    args:
      label: "END"

### SIMULATOR CONFIGURATION ###
simulator_class_path: "usersimcrs.simulator.llm.simple_prompt_user_simulator.SinglePromptUserSimulator"
simulator_id: "Single_LLM_nemo_ReDial"

domain: data/domains/redial.yaml
item_type: "movie"

collection_db_path: "data/item_collections.db"
collection_name: "redial_movies"
items: data/item_collections/redial/movies.csv
id_col: item_id
domain_mapping:
  title:
    slot: TITLE
  year:
    slot: YEAR
ratings: data/item_collections/redial/ratings.csv
historical_ratings_ratio: 0.8

task_definition: "You are a USER discussing with an ASSISTANT. Your goal is to get a recommendation according to the REQUIREMENTS.\nABANDONMENT CONDITIONS: You MUST abandon the conversation by sending the token '\\giveup' if you encounter any of the following failure modes:\n* The ASSISTANT provides repetitive or circular responses\n* The ASSISTANT is unable to understand or act on a simple, direct request\n* You have made multiple attempts to clarify a request, and the ASSISTANT continues to fail\n* The conversation has reached a logical dead end with no clear path forward\nYou should fulfill all REQUIREMENTS as the conversation progresses (you don't need to fulfill them all at once).\nTERMINATION CONDITION: You MUST terminate the conversation by sending '\\end' once you fulfill the REQUIREMENTS.\nGiven the ABANDONMENT conditions, the TERMINATION condition and conversation history, you need to either send specific token or generate the next USER message in the most natural way possible. "

llm_interface_class_path: "usersimcrs.simulator.llm.interfaces.ollama_interface.OllamaLLMInterface"
llm_interface_args:
  configuration_path: config/llm_interface/config_ollama_default.yaml
